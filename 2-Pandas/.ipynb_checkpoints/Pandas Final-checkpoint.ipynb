{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbiUZDMRYVi9"
   },
   "source": [
    "# Pandas\n",
    "\n",
    "- Author:Varun Anand Patkar\n",
    "- Created On : 14 Nov 2020\n",
    "\n",
    "**<a href=\"https://colab.research.google.com/drive/1bb_TkR78I2TSM9EXFfo6Mq4uvFDfqpmK?usp=sharing\">Collab Notebook Link</a>**\n",
    " \n",
    "Now let's download a dataset. Today we are going to be using Stack Overflow's 2020 Survey data as our dataset.\n",
    "\n",
    "Here's the link to check out some insights and all previous year datasets:\n",
    "<br><a href=\"https://insights.stackoverflow.com/survey\">Stack Overflow Dataset and Previous Year link</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77I2ECoUYVjE"
   },
   "outputs": [],
   "source": [
    "#Download files\n",
    "!rm -rdf *\n",
    "!pip install gdown\n",
    "!gdown https://drive.google.com/uc?id=1dfGerWeWkcyQ9GX9x20rdSGj7WtEpzBB\n",
    "!unzip developer_survey_2020.zip\n",
    "!rm developer_survey_2020.zip README_2020.txt so_survey_2020.pdf\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zoNCJissYVjH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHnFjASSYVjJ"
   },
   "source": [
    "## Basic Function\n",
    "\n",
    "- Importing data from csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read data from different file formats, there are many functions. Our dataset is in csv format but we'll see all the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C7Y7_Bm5YVjK"
   },
   "outputs": [],
   "source": [
    "#pd.read_excel('file_name.xlsx',index_col=0) #for reading excel files\n",
    "#pd.read_json('file_name.json') #for reading json files\n",
    "#pd.read_html('file_name.html') #for reading html files\n",
    "#pd.read_sql('file_name.sql') #for reading sql files\n",
    "#pd.read_pickle('file_name.pkl') #for reading .pkl files(pickled pandas files)\n",
    "df=pd.read_csv('survey_results_public.csv') #assigns read data to variable df(stands for dataframe)\n",
    "df.head() #prints first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so why use pandas? We can use native python for this right? \n",
    "\n",
    "Well in native python to read in a csv file, you have to use he csv module to create a csvreader that takes many lines of code and increases complexity. In pandas, you can do all that in one line and faster than native python.\n",
    "\n",
    "To get the total size of your dataframe, you can use the shape attribute of dataframes. And to get the total nos of values in the dataframe use the size attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T4h3NXFsYVjO"
   },
   "outputs": [],
   "source": [
    "print(\"Shape of DataFrame : \"+str(df.shape)+\"\\nTotal size of Dataframe : \"+str(df.size)+\" in bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get more information on each column's data type etc., You can use the info function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7XZryp_MYVjQ"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see all the columns instead of just 20 columns you can use the set_option function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8WV-89mYYVjT"
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\",61)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the first n rows use the head function and the last n use the tail function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ueRLISBWYVjX"
   },
   "outputs": [],
   "source": [
    "print(\"HEAD : \")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNbR_ApZYVjZ"
   },
   "outputs": [],
   "source": [
    "print(\"TAIL : \")\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ArxwvXb3YVjb"
   },
   "source": [
    "## DataFrames and Series(Selecting rows and columns)\n",
    "\n",
    "In this survey, each row is a person who answered the survey and each column is an answer to a question asked in the survey. Now to know what any question means we can load in the schema dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NmpdTRZ4YVjb"
   },
   "outputs": [],
   "source": [
    "schema_df=pd.read_csv('survey_results_schema.csv')\n",
    "pd.set_option(\"display.max_columns\",3)\n",
    "pd.set_option(\"display.max_rows\",61)\n",
    "schema_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, how would you implement a dataframe in regular python?<br>By using a dictionary andthen making a list of that dictionary to keep multiple values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rv6qA4QqYVje"
   },
   "outputs": [],
   "source": [
    "person={\n",
    "    \"first\":\"Varun\",\n",
    "    \"last\":\"Patkar\",\n",
    "    \"email\":\"abc@gmail.com\"\n",
    "}\n",
    "person# for one person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8RMPtzOYVjg"
   },
   "outputs": [],
   "source": [
    "people={\n",
    "    \"first\":[\"Varun\",\"Shardul\",\"Kaif\"],\n",
    "    \"last\":[\"Patkar\",\"Shroff\",\"Kohari\"],\n",
    "    \"email\":[\"abc@gmail.com\",\"def@student.sfit.ac.in\",\"ghi@student.sfit.ac.in\"]\n",
    "}\n",
    "people #for multiple people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this analogy, keys are the colums and values are rows for each tuple(row).<br>Now, how would you access the emails list? By referencing the email key in the people dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gDPhquruYVji"
   },
   "outputs": [],
   "source": [
    "people[\"email\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can not just load data from csv, html, xlsx files, etc. but we can also import data from a dictionary such as the one defined earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FiUMgp3PYVjl"
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(people) #look at the case of the function(capital d and f)\n",
    "df  #the 0,1,2 are indexes for each rows that are either predefined by pandas or defined by the user.We'll see that later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can similarly access colums by using same format as dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RhtCCVzHYVjn"
   },
   "outputs": [],
   "source": [
    "df['email']; #this is not a list or a dataframe but a series. Let's check it's type\n",
    "print(\"Emails:\\n\",df['email'],\"\\n\\n\\nType:\",type(df['email']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Series is like a list like dataframes are like dictionaries. They act like lists but have more functionality. \n",
    "\n",
    "Series:1D Container\n",
    "<br>Dataframes:2D Container(A container for multiple series objects)\n",
    "\n",
    "**Another way is to use the dot notation(only applicable for single word colums i.e. you can't access \"First Name\" with dot but can access\"First_Name\". This is not an efficient way as there is a chance that a column is named same as a function or atttribute of a dataframe so it is better to use the square bracket way**\n",
    "\n",
    "ex. you have a column named count so you try to write df.count but end up referencing the count attribute of a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OYfblJmgYVjq"
   },
   "outputs": [],
   "source": [
    "df.email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can reference multiple colums by using bracket notation with a list of column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jd2ETlw_YVjs"
   },
   "outputs": [],
   "source": [
    "df[['first','email']]#returns a dataframe not a series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look at all the colums available to us we can use the columns attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJAo2jNfYVjw"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tJ4HGQh_YVjy"
   },
   "outputs": [],
   "source": [
    "df.iloc[[0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dS5rA7zFYVj0"
   },
   "outputs": [],
   "source": [
    "df.iloc[[0,1],[0,1]]# we can then pass in another list of nos that specify the columns you want\n",
    "#(iloc so cant pass in column names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-NIEeVg4YVj2"
   },
   "outputs": [],
   "source": [
    "#In loc we can specify the names of colums(but as we have not given indexes they will remain the same)\n",
    "df.loc[[0,1],\"email\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VC7P7CdkT9eU"
   },
   "outputs": [],
   "source": [
    "# Ex 1\n",
    "#Write a line of code to extract Kaif's First Name and Email only using loc\n",
    "#ADD YOUR CODE HERE\n",
    "\n",
    "#END CODE HERE\n",
    "#Write a line of code to extract Kaif's First and Last Name using iloc\n",
    "#ADD YOUR CODE HERE\n",
    "\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwAigOnLYVj5"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('survey_results_public.csv')\n",
    "schema_df=pd.read_csv('survey_results_schema.csv')\n",
    "pd.set_option(\"display.max_columns\",61)\n",
    "pd.set_option(\"display.max_rows\",61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q3fq_49hYVj7"
   },
   "outputs": [],
   "source": [
    "df[[\"Hobbyist\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we were in native python and wanted to know how many answered yes and no we would have to use a for loop etc.\n",
    "\n",
    "Here's where pandas shines. Let's see hwo to get nos. of Yes and Nos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nMeB6wQ5YVj-"
   },
   "outputs": [],
   "source": [
    "df[\"Hobbyist\"].value_counts() #using the built in value_counts method we can find the nos. of yes and nos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UynJV99-YVj_"
   },
   "outputs": [],
   "source": [
    "#let's try to get the first 100 answers using the loc function\n",
    "\n",
    "#Remember:last value is inclusive unlike native python where last value is excluded\n",
    "\n",
    "df.loc[0:100,\"Hobbyist\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajnPoOwlYVkB"
   },
   "outputs": [],
   "source": [
    "#Similarly to get Hobbyist till CompTotal we cna use slicing\n",
    "\n",
    "#Remember:last value is inclusive unlike native python where last value is excluded\n",
    "\n",
    "df.loc[0:100,\"Hobbyist\":\"CompTotal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inCcBwbeYVkD"
   },
   "source": [
    "## Setting, Resetting and Using Indexes\n",
    "\n",
    "For this let's go back to our old example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nT8zJLOnYVkD"
   },
   "outputs": [],
   "source": [
    "people={\n",
    "    \"First Name\":[\"Varun\",\"Kaif\",\"Shardul\"],\n",
    "    \"Last Name\":[\"Patkar\",\"Kohari\",\"Shroff\"],\n",
    "    \"email\":[\"abc@student.sfit.ac.in\",\"def@student.sfit.ac.in\",\"ghi@student.sfit.ac.in\"]\n",
    "}\n",
    "df=pd.DataFrame(people)\n",
    "df\n",
    "#the default index is the left most column. the default one just gives the integer for each row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now,Indexes are values that you refer each column by. So it needs to be unique. Pandas doesnt check if it is unique but it is better to keep them usiques. In this case, we can use the email as Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbiYW_yEYVkF"
   },
   "outputs": [],
   "source": [
    "df.set_index(\"email\")#Remember, this returns a dataframe but it doesnt save it to df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TMPWG7D3YVkH"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2VkLmreYVkI"
   },
   "outputs": [],
   "source": [
    "#To save it to df, you need to overwrite it. Or you can put the inplace parameter to true so it assigns it directly\n",
    "\n",
    "# df=df.set_index(\"email\")\n",
    "\n",
    "df.set_index(\"email\",inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then access the index column by using the index attribute of a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uzrEhUb5YVkK"
   },
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6mf1M7XFYVkM"
   },
   "outputs": [],
   "source": [
    "#Now we can find the names of people using the email as an index\n",
    "pd.DataFrame(df.loc[\"abc@student.sfit.ac.in\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I_E87QRNYVkO"
   },
   "outputs": [],
   "source": [
    "# We need to remember that we cant use the default integer ones now:\n",
    "# df.loc[0](we can use the iloc to find this in this way)\n",
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you messed up in the index naming? Well, you can reset it to default indexes using the reset_index function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2kzRWxGIYVkP"
   },
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3OoiauVUupm"
   },
   "outputs": [],
   "source": [
    "#Ex 2\n",
    "#Set Last Name as index and extract Shardul's email using loc\n",
    "#ADD YOUR CODE HERE\n",
    "\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load back in the survey data and try this with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zy04WBIQYVkS"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('survey_results_public.csv')\n",
    "schema_df=pd.read_csv('survey_results_schema.csv')\n",
    "pd.set_option(\"display.max_columns\",61)\n",
    "pd.set_option(\"display.max_rows\",61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xKse43n6YVkT"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gyNjZ1NuYVkV"
   },
   "outputs": [],
   "source": [
    "#We can see that there is already a Respondent index unique for each row\n",
    "#We can do it the way that we saw earlier or just do it while reading in the data\n",
    "df=pd.read_csv('survey_results_public.csv',index_col=\"Respondent\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if i wanted to see what a question is for a column without looking through schema manually, What can i do?\n",
    "\n",
    "We can set the Column name as the index so we can use the loc function to take in each question name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KyAJ0kE7YVkY"
   },
   "outputs": [],
   "source": [
    "schema_df=pd.read_csv('survey_results_schema.csv',index_col=\"Column\")\n",
    "schema_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LgIAlyxpYVkc"
   },
   "outputs": [],
   "source": [
    "schema_df.loc[\"Hobbyist\",\"QuestionText\"]#now i can reference each question like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X1l0qYFUYVke"
   },
   "outputs": [],
   "source": [
    "#To see them in alphabetical order we can use the sort_index function\n",
    "schema_df.sort_index()#usie inplace to make it permanent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EUuo0NN-YVkg"
   },
   "source": [
    "## Filtering Rows and Columns using Conditionals\n",
    "\n",
    "We can create a filter or a mask for a certain value/s using standard python conditionals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E0UlP8wfYVkh"
   },
   "outputs": [],
   "source": [
    "people={\n",
    "    \"First Name\":[\"Varun\",\"Kaif\",\"Shardul\"],\n",
    "    \"Last Name\":[\"Patkar\",\"Kohari\",\"Shroff\"],\n",
    "    \"email\":[\"abc@student.sfit.ac.in\",\"def@student.sfit.ac.in\",\"ghi@student.sfit.ac.in\"]\n",
    "}\n",
    "df=pd.DataFrame(people)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S1PIfeZLYVkj"
   },
   "outputs": [],
   "source": [
    "#dont use filter(keyword in python)\n",
    "filt=(df[\"Last Name\"]==\"Patkar\")\n",
    "df[\"Last Name\"]==\"Patkar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIvo2jRqYVkk"
   },
   "outputs": [],
   "source": [
    "df[filt]#this way we can generate and apply filters to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fi7GnMCfYVkl"
   },
   "outputs": [],
   "source": [
    "#We can do this same thing using the loc function\n",
    "df.loc[filt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G9TffwhjYVkn"
   },
   "outputs": [],
   "source": [
    "#Using the loc we can also specify the columns \n",
    "df.loc[filt,[\"First Name\",\"Last Name\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ss-Yb9gRYVkp"
   },
   "source": [
    "There is one question though. What about and & or operators?\n",
    "\n",
    "You can't use the builtin and & or operators but we can use:<br>**& : and<br>| : or**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5EbxbtoYVkp"
   },
   "outputs": [],
   "source": [
    "filt=((df['Last Name']==\"Patkar\") | (df['Last Name']==\"Kohari\"))\n",
    "filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bstoi1nnYVkr"
   },
   "outputs": [],
   "source": [
    "df.loc[filt,[\"email\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gz6lwhxLYVkt"
   },
   "outputs": [],
   "source": [
    "filt=(((df['First Name']==\"Varun\") | (df['First Name']==\"Shardul\")) & ((df['Last Name']==\"Patkar\") | (df['Last Name']==\"Shroff\")))\n",
    "filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gIxy_7dCYVkw"
   },
   "outputs": [],
   "source": [
    "df.loc[filt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the rejected ones by adding a NOT(~, tilda, left of your 1 and below your esc) in front of filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-kWZC3tYVky"
   },
   "outputs": [],
   "source": [
    "df.loc[~filt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load in our survey data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cIGVGx8YVkz"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('survey_results_public.csv',index_col=\"Respondent\")\n",
    "schema_df=pd.read_csv('survey_results_schema.csv',index_col=\"Column\")\n",
    "pd.set_option(\"display.max_columns\",61)\n",
    "pd.set_option(\"display.max_rows\",61)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have to see which programming language makes the most money.\n",
    "\n",
    "To get the people with salary above a set amt we can use this. Remember, ConvertedComp contains the salary of individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJ7CjLCcYVk3"
   },
   "outputs": [],
   "source": [
    "salary_filt=(df[\"ConvertedComp\"]>70000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bh2L-JWkYVk4"
   },
   "outputs": [],
   "source": [
    "df.loc[salary_filt,[\"Country\",\"LanguageWorkedWith\",\"ConvertedComp\"]]#Let's only get the country,Languages used and salary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's say we have a list of countries that we care about only. We can use the isin function to filter that out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNBzNoEMYVk6"
   },
   "outputs": [],
   "source": [
    "countries=[\"India\",\"United States\",\"United Kingdom\",\"Canada\",\"Germany\"]\n",
    "country_filter=df[\"Country\"].isin(countries)\n",
    "pd.DataFrame(df.loc[country_filter,\"Country\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use string manipulation like in python here. Let's say you want to find out each person that has worked with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wGdLo_vdYVk9"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(df[\"LanguageWorkedWith\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that the languages are seperated by semicolons. so cant do == \"Python\".\n",
    "\n",
    "Instead we will use string manipulation. We use .str.*** functions that are called as string functions in pandas.\n",
    "\n",
    "Here are stringmethods for pandas: <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html\">StringMethods Pandas Link</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2AOKRmEUYVk_"
   },
   "outputs": [],
   "source": [
    "python_filt=df[\"LanguageWorkedWith\"].str.contains(\"Python\",na=False)#the na is what to put if NaN is encountered\n",
    "pd.DataFrame(df[\"LanguageWorkedWith\"].str.contains(\"Python\",na=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-0YPTBj9YVlB"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(df.loc[python_filt,\"LanguageWorkedWith\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AoxiQOVgXxjK"
   },
   "outputs": [],
   "source": [
    "# Ex 3\n",
    "#Create a filter of people from India that are a Hobbyist(use isin. dont use ==)\n",
    "#ADD YOUR CODE HERE\n",
    "\n",
    "#END CODE HERE\n",
    "#There are 6645 people. check those no of rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vN8up57YVlD"
   },
   "source": [
    "## Modifying/Editing Data with DataFrames\n",
    "### Updating Data in Column Names(Shorter Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kMvJ6oYGYVlD"
   },
   "outputs": [],
   "source": [
    "people={\n",
    "    \"First Name\":[\"Varun\",\"Kaif\",\"Shardul\"],\n",
    "    \"Last Name\":[\"Patkar\",\"Kohari\",\"Schroff\"],\n",
    "    \"email\":[\"abc@student.sfit.ac.in\",\"def@student.sfit.ac.in\",\"ghi@student.sfit.ac.in\"]\n",
    "}\n",
    "df=pd.DataFrame(people)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can rename columns using assignment to df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pdKbS-gcYVlG"
   },
   "outputs": [],
   "source": [
    "df.columns=[\"First_Name\",\"Last_Name\",\"email\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Gl0tMLpYVlH"
   },
   "outputs": [],
   "source": [
    "df.columns=[x.upper() for x in df.columns]#we can also use list comprehension to make it all upper case, etc.\n",
    "print(df)\n",
    "df.columns=[\"First Name\",\"Last Name\",\"email\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MBfg2onzYVlJ"
   },
   "outputs": [],
   "source": [
    "df.columns=[\"First Name\",\"Last Name\",\"email\"]\n",
    "#We can also change the spaces to underscores using the string manip function discussed earlier\n",
    "df.columns=df.columns.str.replace(\" \",\"_\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9alDx6EYVlK"
   },
   "outputs": [],
   "source": [
    "df.columns=[\"first_name\",\"last_name\",\"email\"]# Resetting all changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To only change some of the colums we can use the replace method and pass in a dictionary where keys are original colums and values are new names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WCdCJeVWYVlL"
   },
   "outputs": [],
   "source": [
    "df.rename(columns={\"first_name\":\"First_Name\",\n",
    "                  \"last_name\":\"Last_Name\",\n",
    "                  \"email\":\"Email\"},inplace=True)#do not forget inplace to directly assign output\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxkLHGl7YVlN"
   },
   "source": [
    "### Updating Data in Rows(Shorter Example)\n",
    "\n",
    "Now, as you can see, I have made a mistake in writing Shardul's surname. It is Shroff not Schroff. So how do we edit that?\n",
    "\n",
    "Well, we can just use assignment on the 2nd row using loc function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0lbFw50YVlN"
   },
   "outputs": [],
   "source": [
    "df.loc[2]=[\"Shardul\",\"Shroff\",\"ghi@student.sfit.ac.in\"] \n",
    "#instead of using the index in a large dataset wecan also use conditionals to search for that entry\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, What if we want to just change the last name and not pass all the other values here? Like if there were 100 columns then you would have to list each in the above way. So how do we solve this?\n",
    "\n",
    "We can specify the column in the loc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4AdcCfnZYVlR"
   },
   "outputs": [],
   "source": [
    "df.loc[2,\"Last_Name\"]=\"Schroff\"#Resetting data\n",
    "df #Original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lzzfo16VYVlS"
   },
   "outputs": [],
   "source": [
    "df.loc[2,\"Last_Name\"]=\"Shroff\"#We can also list multiple things and pass in a list in the order to change multiple rows\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vir2dMpnYVlT"
   },
   "source": [
    "Note:If there is a just a single value to be changed, You can use .at function. This is for performance reasons and gives better efficiency.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1Hk_ZQt4Dvq7o_iNEFqw_r24smwWWlHX1\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J88XiB5gYVlU"
   },
   "outputs": [],
   "source": [
    "df.at[2,\"Last_Name\"]=\"Shroff\"\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2weePmJUYVlV"
   },
   "source": [
    ">⚠️ There is a common mistake that people make when using filters on large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lj-vGxvPYVlV"
   },
   "outputs": [],
   "source": [
    "filt=(df['Email']==\"ghi@student.sfit.ac.in\")\n",
    "df[filt]['Last_Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zg0_64fnYVlX"
   },
   "outputs": [],
   "source": [
    "# df[filt]['Last_Name']=\"Schroff\"\n",
    "#Someone can tey to use a filter on df and then try to reinitialise a value like this. This gives a SettingWithCopy warning\n",
    "#This does not change the original dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason for this above warning is that when we apply a filter we make a copy of original dataframe and show it to user. It is a copy not the original dataframe and when you do it like this, it doesnt translate the changes to original dataframe\n",
    "\n",
    "You can take a look at what returning a view vs a copy of dataframe is in pandas here:<a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\">Returning VIew vs Copy</a>\n",
    "\n",
    "Here is how you can do the filtered assignment right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eLQTo2CjYVlZ"
   },
   "outputs": [],
   "source": [
    "df.loc[2,\"Last_Name\"]=\"Schroff\"#Resetting values\n",
    "df#Original value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2B9bOJQYVla"
   },
   "outputs": [],
   "source": [
    "filt=(df['Email']==\"ghi@student.sfit.ac.in\")\n",
    "df.loc[filt,\"Last_Name\"]=\"Shroff\"\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sDCMXafpYVlc"
   },
   "outputs": [],
   "source": [
    "df['Email'] = [\"Varun_Patkar@student.sfit.ac.in\",\"Kaif_Kohari@student.sfit.ac.in\",\"Shardul_Shroff@student.sfit.ac.in\"]\n",
    "#We can use assignment to assign each value like this\n",
    "#Not our actual email. Don't use these.\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IQU8kF6XYVld"
   },
   "outputs": [],
   "source": [
    "#We can also use the string methods\n",
    "df['Email']=df['Email'].str.lower()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nr1ImWx7YVle"
   },
   "source": [
    "Now we will go a bit advanced. There are 4 methods that can be confusing so we will go over them one by one:\n",
    "\n",
    "**1. apply**<br>\n",
    "**2. map**<br>\n",
    "**3. applyman**<br>\n",
    "**4. replace**\n",
    "\n",
    "First let's look at apply for a Series object. We have a len function predefined so lets apply that to email column to find the lengths of each email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "17Ydb86HYVle"
   },
   "outputs": [],
   "source": [
    "df[\"Email\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buLugRcMYVlh"
   },
   "outputs": [],
   "source": [
    "#Now let's try a user defined function\n",
    "#Note: that the function can be as complex as possible. For simplicity  \n",
    "def camel_case(email):\n",
    "    return email[0].upper()+email[1:email.index(\"_\")+1]+email[(email.index(\"_\")+1)].upper()+email[(email.index(\"_\")+2):]\n",
    "df[\"Email\"]=df[\"Email\"].apply(camel_case)#note:we are passing in the function so parenthesis are not needed\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLaueS_sYVli"
   },
   "outputs": [],
   "source": [
    "#We can also put in simple lambda functions here instead of defining functions \n",
    "df[\"Email\"]=df[\"Email\"].apply(lambda x:x.lower())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-sdNGnIYVlj"
   },
   "outputs": [],
   "source": [
    "df[\"Email\"]=df[\"Email\"].apply(camel_case)#Resetting Values\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j4GajuGpYVll"
   },
   "outputs": [],
   "source": [
    "#We can do it on numbers also\n",
    "nos={\n",
    "    \"n\":list(range(0,101))\n",
    "}\n",
    "nos=pd.DataFrame(nos)\n",
    "nos[\"n^2\"]=nos[\"n\"].apply(lambda x:(x*x))\n",
    "nos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4B3EIOpSYVlm"
   },
   "outputs": [],
   "source": [
    "df.apply(len) #this applies len function to each column(gives 3 as it gives len of list in each column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-bZoSfdYYVln"
   },
   "outputs": [],
   "source": [
    "#it's like\n",
    "print(len(df[\"First_Name\"]))\n",
    "print(len(df[\"Last_Name\"]))\n",
    "print(len(df[\"Email\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q1WdPnhMYVlp"
   },
   "outputs": [],
   "source": [
    "print(df.apply(len,axis=\"columns\"))#we can also set the axis to rows or columns\n",
    "print(df.apply(len,axis=\"rows\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oh5DBqVaYVlq"
   },
   "outputs": [],
   "source": [
    "#there is a series min function. as we have string values, it gives the minimum in alphabetical order\n",
    "print(df.apply(pd.Series.min,axis=\"columns\"))\n",
    "print(df.apply(pd.Series.min,axis=\"rows\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AM9wSv2zYVlr"
   },
   "outputs": [],
   "source": [
    "print(nos.apply(pd.Series.mean)) #calculates avg of each vertical column\n",
    "#Can alsouse numpy functions\n",
    "print(nos.apply(lambda x:np.sqrt(x)))#gives series output to series input so full output is a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6s0BFuUgnlS"
   },
   "source": [
    "**So apply on a series applies function to each element and on a dataframe applies function to every series in the DF**\n",
    "\n",
    "But can we apply a function to each element in a dataframe? That is what applymap is used for(applymap is only for dataframes. not series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4YWqyeSsYVlt"
   },
   "outputs": [],
   "source": [
    "df.applymap(len) #applies len function to each and every element in DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LUF2DoR6YVlu"
   },
   "outputs": [],
   "source": [
    "#we can use this to apply string methods to each element in DF\n",
    "df.applymap(str.lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIHagbt_YVlv"
   },
   "source": [
    "**Now let's look at a map. Map is only applicable for a series. Used for mapping each value in a series to another value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6v9ZQscCYVlw"
   },
   "outputs": [],
   "source": [
    "df[\"First_Name\"].map({\"Kaif\":\"Harsh\",\"Shardul\":\"Sneh\"}) \n",
    "#Now you see that the values that i did not put in i.e. my name is written as NaN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we want to keep the other values that we did not put in then we can put in the replace method. THe replace method replaces the valeus given and lets others remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NohHqumFYVlx"
   },
   "outputs": [],
   "source": [
    "df[\"First_Name\"].replace({\"Kaif\":\"Harsh\",\"Shardul\":\"Sneh\"}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Ntgy_MLYVlz"
   },
   "outputs": [],
   "source": [
    "df[\"First_Name\"]=df[\"First_Name\"].replace({\"Kaif\":\"Harsh\",\"Shardul\":\"Sneh\"}) \n",
    "df[\"Last_Name\"]=df[\"Last_Name\"].replace({\"Kohari\":\"Malhotra\",\"Shroff\":\"Modi\"}) \n",
    "df[\"Email\"]=df[\"Email\"].replace({\"Kaif_Kohari@student.sfit.ac.in\":\"Harsh_Malhotra@student.sfit.ac.in\",\"Shardul_Shroff@student.sfit.ac.in\":\"Sneh_Modi@student.sfit.ac.in\"}) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sINKB_YmYVl1"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('survey_results_public.csv',index_col=\"Respondent\")\n",
    "schema_df=pd.read_csv('survey_results_schema.csv',index_col=\"Column\")\n",
    "pd.set_option(\"display.max_columns\",61)\n",
    "pd.set_option(\"display.max_rows\",61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lq4FE6CYYVl5"
   },
   "outputs": [],
   "source": [
    "df.rename(columns={\"ConvertedComp\":\"SalaryUSD\"},inplace=True)\n",
    "#At first dont put inplace and check if correct. Only then do inplace=True\n",
    "df[\"SalaryUSD\"]#it looks like nan. but when we look at the valuecounts it is clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DjPSu4TKYVl7"
   },
   "outputs": [],
   "source": [
    "df[\"SalaryUSD\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwj4Iu-hYVl8"
   },
   "outputs": [],
   "source": [
    "#Now let's try to map the Hobbyist column to Yes->True and No->False\n",
    "df[\"Hobbyist\"].map({\"Yes\":True,\"No\":False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcs2GpgBYVl9"
   },
   "outputs": [],
   "source": [
    "df[\"Hobbyist\"] = df[\"Hobbyist\"].replace({\"Yes\":True,\"No\":False})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hzAOW9idd1qq"
   },
   "outputs": [],
   "source": [
    "#Ex 4\n",
    "#Replace in Country column India with IN,United States with USA and United Kingdom with UK\n",
    "#Then create a filter that gives True for IN,USA and UK and then apply it in df and see if your output is correct\n",
    "#ADD YOUR CODE HERE\n",
    "\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mx7OMI96YVl-"
   },
   "source": [
    "## Add/ Remove/ Combine Columns and Rows from DataFrame\n",
    "### Adding a column\n",
    "\n",
    "Note:Can't use dot notation when creating a column. Needs to be in brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DL67zh1fYVl-"
   },
   "outputs": [],
   "source": [
    "people={\n",
    "    \"First_Name\":[\"Varun\",\"Kaif\",\"Shardul\"],\n",
    "    \"Last_Name\":[\"Patkar\",\"Kohari\",\"Shroff\"],\n",
    "    \"Email\":[\"abc@student.sfit.ac.in\",\"def@student.sfit.ac.in\",\"ghi@student.sfit.ac.in\"]\n",
    "}\n",
    "df=pd.DataFrame(people)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wdJUcZkNYVmA"
   },
   "outputs": [],
   "source": [
    "df[\"First_Name\"]+\" \"+df[\"Last_Name\"]#Generates series of names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_ynww6RYVmB"
   },
   "outputs": [],
   "source": [
    "#To create a column just make a new entry \"Full Name\" and it will add to it\n",
    "df[\"Full_Name\"]=df[\"First_Name\"]+\" \"+df[\"Last_Name\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3jLxJQ2YVmC"
   },
   "source": [
    "### Removing Column(Using Drop Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8qp2xKGZYVmD"
   },
   "outputs": [],
   "source": [
    "df.drop(columns=[\"First_Name\",\"Last_Name\"],inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to split full name column back to first and last names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1uFCZ0UBYVmE"
   },
   "outputs": [],
   "source": [
    "df[\"Full_Name\"].str.split(\" \")#gives in same series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "02CB4fMpYVmF"
   },
   "outputs": [],
   "source": [
    "#To make this into 2 series we use expand parameter\n",
    "df[\"Full_Name\"].str.split(\" \",expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rri9TbAHYVmG"
   },
   "outputs": [],
   "source": [
    "#Now assign it to 2 different columns\n",
    "df[[\"First_Name\",\"Last_Name\"]]=df[\"Full_Name\"].str.split(\" \",expand=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VeNUVYQdYVmH"
   },
   "source": [
    "### Adding a Single Row(Using Append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajFr7lR_YVmI"
   },
   "outputs": [],
   "source": [
    "# df.append({\"First_Name\":\"Sneh\",\"Last_Name\":\"Modi\",\"Full_Name\":\"Sneh Modi\",\"Email\":\"jkl@student.sfit.ac.in\"})\n",
    "# Gives error. Let's set ignore index to true and it will automatically figure it out\n",
    "#The eroor is because we have passed an unnamed Series(converts dict to Series) and index is default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_39twPmhYVmJ"
   },
   "outputs": [],
   "source": [
    "df=df.append({\"First_Name\":\"Sneh\",\"Last_Name\":\"Modi\",\"Full_Name\":\"Sneh Modi\",\"Email\":\"jkl@student.sfit.ac.in\"},ignore_index=True)\n",
    "df\n",
    "#Note:No inplace parameter. Have to use assignment here\n",
    "#If you dont give any values NaN will be filled in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lyf-ux6dYVmM"
   },
   "outputs": [],
   "source": [
    "people={\n",
    "    \"First_Name\":[\"Harsh\",\"Ekta\"],\n",
    "    \"Last_Name\":[\"Malhotra\",\"Masrani\"],\n",
    "    \"Email\":[\"mno@student.sfit.ac.in\",\"pqr@student.sfit.ac.in\"]\n",
    "}\n",
    "df2=pd.DataFrame(people)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLGYab1CYVmN"
   },
   "outputs": [],
   "source": [
    "df = df.append(df2,ignore_index=True)#Full_Name is not there so it will remain NaN\n",
    "df\n",
    "#may give sorting error in old version of pandas. no need to worry. Just out sort=False in older pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XSI6FBtYVmO"
   },
   "source": [
    "### Removing Rows(with drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4owUtzHYVmO"
   },
   "outputs": [],
   "source": [
    "#Let's try to remove 3-5 rows\n",
    "df.drop(index=[3,4,5])#No inplace so no change to df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VupVjnmqYVmP"
   },
   "source": [
    "### Removing Rows with conditional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TIIfMEA0YVmP"
   },
   "outputs": [],
   "source": [
    "#Let's try to remove all people with full_name==NaN\n",
    "check_null=(pd.isnull(df[\"Full_Name\"]))\n",
    "df.drop(index=df[check_null].index,inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQFnfx7YYVmQ"
   },
   "source": [
    "## Sorting Data in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmS1Kk27YVmQ"
   },
   "outputs": [],
   "source": [
    "people={\n",
    "    \"First_Name\":[\"Varun\",\"Kaif\",\"Shardul\"],\n",
    "    \"Last_Name\":[\"Patkar\",\"Kohari\",\"Shroff\"],\n",
    "    \"Email\":[\"abc@student.sfit.ac.in\",\"def@student.sfit.ac.in\",\"ghi@student.sfit.ac.in\"]\n",
    "}\n",
    "df=pd.DataFrame(people)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v70rFgaTYVmR"
   },
   "outputs": [],
   "source": [
    "df.sort_values(by=\"Last_Name\",ascending=True)#sort by last name in ascending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qCAC6ZO9YVmS"
   },
   "outputs": [],
   "source": [
    "df.sort_values(by=\"Last_Name\",ascending=False)#sort by last name in descending order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edB6rRuiYVmT"
   },
   "source": [
    "But what if 2 elements have same value. Then we will have to sort by another column. So we can pass in list. So if first is same then check 2nd, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4FlvP-x4YVmT"
   },
   "outputs": [],
   "source": [
    "df=df.append({\"First_Name\":\"Varun\",\"Last_Name\":\"Patkar1\",\"Email\":\"jkl@student.sfit.ac.in\"},ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p_6Mlra2YVmU"
   },
   "outputs": [],
   "source": [
    "df.sort_values(by=[\"First_Name\",\"Last_Name\"])#First check first name and if same then check last name\n",
    "#To make changes permanent use inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLlTvEjnYVmV"
   },
   "outputs": [],
   "source": [
    "df.sort_values(by=[\"First_Name\",\"Last_Name\"],ascending=[True,False])\n",
    "#First check first name and srt in ascending but if same then check last name and sort last names in descending order\n",
    "#To make changes permanent use inplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we want ot sort by index we can use sort_index function.\n",
    "\n",
    "Note:This is the default index so will sort in order of addition/definition. If we set index as some other column, that will be reflected according to that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywj7H7Y2YVmW"
   },
   "outputs": [],
   "source": [
    "df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mUls866yYVmY"
   },
   "outputs": [],
   "source": [
    "#To only sort a series you can slice the dataframe and then sort it\n",
    "df[\"Last_Name\"].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJ0cJtovYVmZ"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('survey_results_public.csv',index_col=\"Respondent\")\n",
    "schema_df=pd.read_csv('survey_results_schema.csv',index_col=\"Column\")\n",
    "pd.set_option(\"display.max_columns\",61)\n",
    "pd.set_option(\"display.max_rows\",61)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X_Smu6QSYVma"
   },
   "outputs": [],
   "source": [
    "#Let's sort these by country name and then by salary\n",
    "df.sort_values(by=\"Country\",inplace=True)\n",
    "df[[\"Country\",\"ConvertedComp\"]].head(50)\n",
    "#See that salary is not sorted. But to see the top salaries we can sort in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_8F-PWSYVmb",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.sort_values(by=[\"Country\",\"ConvertedComp\"],ascending=[True,False],inplace=True)\n",
    "df[[\"Country\",\"ConvertedComp\"]].head(50)\n",
    "#You can see that there are many outliers that make 1 million USD. We can account for these in Aggregation Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JTZtI2ECYVmc"
   },
   "outputs": [],
   "source": [
    "df[\"ConvertedComp\"].nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGrZsnQQYVmd"
   },
   "outputs": [],
   "source": [
    "#To see other values like country also, you can use it like this\n",
    "df.nlargest(10,\"ConvertedComp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7O8Hj_ELYVme"
   },
   "outputs": [],
   "source": [
    "#Similarly\n",
    "df.nsmallest(10,\"ConvertedComp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHDQG-bQYVmf"
   },
   "source": [
    "## Analyzing and Exploring Data(Grouping and Aggregating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6ZZZn54YVmf"
   },
   "source": [
    "### Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GHHttPtIYVmf"
   },
   "outputs": [],
   "source": [
    "#Reloading Survey Data\n",
    "df=pd.read_csv('survey_results_public.csv',index_col=\"Respondent\")\n",
    "schema_df=pd.read_csv('survey_results_schema.csv',index_col=\"Column\")\n",
    "pd.set_option(\"display.max_columns\",61)\n",
    "pd.set_option(\"display.max_rows\",61)\n",
    "df[\"Gender\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregation is compiing several peices of data into single result.<br>Ex:count,sum,avg,RMS, etc. wherever we take in a bunch of data and give single value as output\n",
    "\n",
    "Now let's say you want to find what median salary is for the given data. Here's how to find it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQhjHt7UYVmh"
   },
   "outputs": [],
   "source": [
    "df[\"ConvertedComp\"].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we know each country has different economy and so has different value of currency. So this doesnt reflect that properly. So it would be better to look atmedian salary per country. That will be done in Groupinng later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNxNrwTnYVmj"
   },
   "outputs": [],
   "source": [
    "df.median()\n",
    "#if applied on whole dataframe, scans for all numeric values and finds their medians and prints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NaV-wgKkYVmk"
   },
   "outputs": [],
   "source": [
    "#To get a statistical description of data we can use the describe function\n",
    "df.describe()\n",
    "#Note:25% is quarter quantile,50% is median and 75% is three quarters quantile.\n",
    "#count is number of non NaN(not a number) values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dmuqLm7YVmm"
   },
   "source": [
    "**Note:We used median as median is a good metric when measuring quantities as median is not much affected by outliers. A small no of outliers affect mean greately but dont affect median as much. Look at the mean and 50% above to see how**\n",
    "\n",
    "**Note:Sometimes people confuse count with a function that counts nos of values in a row and reports for each column. This is what value_counts is used for**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zjuz5M8SYVmm"
   },
   "outputs": [],
   "source": [
    "df[\"Hobbyist\"].count()#Returns total nos of YES and NOS(here everyone answered so there is no null value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KfTHPkSnYVmn"
   },
   "outputs": [],
   "source": [
    "df[\"Hobbyist\"].value_counts()#Counts individual nos of each option and shows them for the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fCbMuAnnYVmp"
   },
   "outputs": [],
   "source": [
    "df[\"SurveyLength\"].value_counts()#Shows How many and what people thought of the length of the survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wlmKiUseYVmq"
   },
   "outputs": [],
   "source": [
    "df[\"SurveyLength\"].value_counts(normalize=True)*100 #Normalize gives percent/100\n",
    "#Similarly each country has different people with opinions then we will have to group by the country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86fWvilRYVmr"
   },
   "source": [
    "### Grouping(using groupby function)\n",
    "\n",
    "**There are 3 steps in Grouping:Splitting the object,applying the given function to each and then combining the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0M0s6rMYVmr"
   },
   "outputs": [],
   "source": [
    "df[\"Country\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYc5s6bAYVms"
   },
   "outputs": [],
   "source": [
    "country_group=df.groupby([\"Country\"])#Returns a FataFrameGroupBy object\n",
    "#DataFrameGroupBy object contains a bunch of groups of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ta8VuOgFYVmt"
   },
   "outputs": [],
   "source": [
    "country_group.get_group(\"India\")#Gives all the people from India that filled the form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now getting a group is nothing surprising as we could have done it using filters and checked if country is India. But this splits it all into each entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zvCMN4F4YVmu"
   },
   "outputs": [],
   "source": [
    "country_group.get_group(\"India\")[\"Hobbyist\"].value_counts()\n",
    "#Then we can just apply the value counts to the hobbyist section. This gives hobbyist detais of only India."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwUHGHUgYVmv"
   },
   "outputs": [],
   "source": [
    "#Now, let's run this on the groupby object\n",
    "countries_hobbyist=country_group[\"Hobbyist\"].value_counts(normalize=True)*100#Gives the Hobbyist answer by country\n",
    "countries_hobbyist.head(60)\n",
    "#We are only seeing the first 60.\n",
    "#There are total 183 countries and some have not answered so there are total 333 yes/no answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yTgTAqR4YVmw"
   },
   "outputs": [],
   "source": [
    "#Now we can access each country by saving it into a variable\n",
    "countries_hobbyist.loc[\"India\"]\n",
    "#This series has multiple indexes which you can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bKsrlsqUYVmx"
   },
   "outputs": [],
   "source": [
    "countries_hobbyist.loc[\"United States\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now getting a group is nothing surprising as we could have done it using filters and checked if country is India. But this splits it all into each entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XjH-MjQPYVmy"
   },
   "outputs": [],
   "source": [
    "country_group[\"ConvertedComp\"].median()#Medain salary by Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uMU7CoIQYVm1"
   },
   "outputs": [],
   "source": [
    "#We cam now make requests to some country easily\n",
    "print(\"Median Salary(In India) = $\",country_group[\"ConvertedComp\"].median().loc[\"India\"])\n",
    "print(\"Median Salary(In USA) = $\",country_group[\"ConvertedComp\"].median().loc[\"United States\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8XP8t5BlYVm2"
   },
   "outputs": [],
   "source": [
    "country_group[\"ConvertedComp\"].agg([\"median\",\"mean\",\"count\"])\n",
    "#The agg function takes in multiple functions and applies each to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ztJT48Y2YVm3"
   },
   "outputs": [],
   "source": [
    "#Similarly\n",
    "country_group[\"ConvertedComp\"].agg([\"median\",\"mean\",\"count\"]).loc[\"India\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tdozeyz-YVm6"
   },
   "outputs": [],
   "source": [
    "country_group[\"ConvertedComp\"].agg([\"median\",\"mean\",\"count\"]).loc[\"United States\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yoezp1voYVm7"
   },
   "outputs": [],
   "source": [
    "#Let's try to find how any people use Python by country\n",
    "# country_group[\"LanguageWorkedWith\"].str.contains(\"Python\").sum()\n",
    "#Gives error as stringmethods cant be applied to SeriesGroupBy object as group by are multiple. \n",
    "#So we need to use the apply function instead to apply the string methods to each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a-iy_p3JYVm7"
   },
   "outputs": [],
   "source": [
    "country_group[\"LanguageWorkedWith\"].apply(lambda x:x.str.contains(\"Python\"))\n",
    "#Gives true and false values. Need to sum them to find nos of trues in each coutnry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "guOIuKlCYVm9"
   },
   "outputs": [],
   "source": [
    "country_python=country_group[\"LanguageWorkedWith\"].apply(lambda x:x.str.contains(\"Python\").sum())\n",
    "country_python\n",
    "#this gives nos. But we can't say anything with that. But if we have percentage then we can say easily how many used python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EgvYT0GzYVm-"
   },
   "outputs": [],
   "source": [
    "country_participants=df[\"Country\"].value_counts()\n",
    "#Now let's match up both indexes. To do this we need t use the pandas concat function to match up indexes side by side\n",
    "python_df=pd.concat([country_python,country_participants],axis=\"columns\")\n",
    "python_df.rename(columns={\"Country\":\"Participants\",\"LanguageWorkedWith\":\"UsedPython\"},inplace=True)\n",
    "python_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GerExsldYVm_"
   },
   "outputs": [],
   "source": [
    "python_df[\"pctKnowsPython\"]=(python_df[\"UsedPython\"]/python_df[\"Participants\"])*100\n",
    "python_df.sort_values(by=\"pctKnowsPython\",ascending=False,inplace=True)\n",
    "python_df.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0TE9XNS7YVnA"
   },
   "outputs": [],
   "source": [
    "#Then we can access each of them\n",
    "python_df.loc[\"India\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x7JiHY3jYVnA"
   },
   "outputs": [],
   "source": [
    "python_df.loc[\"United States\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sGaQTKEgpfY3"
   },
   "outputs": [],
   "source": [
    "#Ex 5(Hardest and Most Important Exercise)\n",
    "#Let's try to see the ratio of women that know python vs men that know python that gave the survey\n",
    "\n",
    "#Group by Gender and find no of people that know python\n",
    "#then find the percentage of men who know python and women that know Python and print that out\n",
    "#Don't include Non-binary, genderqueer, or gender non-conforming\n",
    "#Then find the ratio of women vs men and put that in chat.\n",
    "\n",
    "#ADD YOUR CODE HERE\n",
    "\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBvuDwT2YVnB"
   },
   "source": [
    "## Working with missing/unknown data in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4Mf-Q_iYVnB"
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(\n",
    "    {\n",
    "        \"First_Name\":[\"Varun\",\"Shardul\",\"Kaif\",\"Sneh\",np.nan,None,\"NA\"],\n",
    "        \"Last_Name\":[\"Patkar\",\"Shroff\",\"Kohari\",\"Modi\",np.nan,np.nan,\"Missing\"],\n",
    "        \"Email\":[\"VarunPatkar@gmail.com\",\"ShardulShroff@gmail.com\",\"KaifKohari@gmail.com\",None,np.nan,\"Anonymous@protonmail.com\",\"NA\"],\n",
    "        \"Age\":[\"19\",\"19\",\"20\",\"20\",None,None,\"Missing\"]\n",
    "    }\n",
    ")#CHANGE AGES LATER\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oC-WBRBCYVnC"
   },
   "outputs": [],
   "source": [
    "#If we just want to remove all rows with NaN or Null use dropna function\n",
    "df.dropna(axis=\"index\",how=\"any\")#the given values of params are default values\n",
    "#Last row remains as it has string values.\n",
    "#axis can be index or columns. index->drop rows,columns->drop columns\n",
    "#how can be any(if any of the row has a missing value, it drops),all(all need to be missing in order to drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j51QZ1LwYVnD"
   },
   "outputs": [],
   "source": [
    "df.dropna(axis=\"index\",how=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdMBhV-nYVnE"
   },
   "outputs": [],
   "source": [
    "df.dropna(axis=\"columns\",how=\"any\")#drops all as all columns have at least 1 missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BmqYRpaLYVnE"
   },
   "outputs": [],
   "source": [
    "df.dropna(axis=\"columns\",how=\"all\")#nothing dropped as no column has all misssing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what if we will allow first and last names but email address is compulsary? SO we can put in a subset argument(Subset is the column names that we check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BOMOz5h8YVnG"
   },
   "outputs": [],
   "source": [
    "df.dropna(axis=\"index\",subset=[\"Email\"])#if we are only having 1 value in subset then any and all both give same output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jm4RAAUZYVnH"
   },
   "outputs": [],
   "source": [
    "#I need last name or email. none will not work. both will work\n",
    "df.dropna(axis=\"index\",how=\"all\",subset=[\"Email\",\"Last_Name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the only problem is the custom missing values like \"NA\" or \"Missing\". First let's look at hwo to handle when preloaded data is there in a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RdqO6PlRYVnI"
   },
   "outputs": [],
   "source": [
    "df.replace(\"NA\",np.nan,inplace=True)#Replace all strings with the NaN value\n",
    "df.replace(\"Missing\",np.nan,inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uuVC4soRYVnI"
   },
   "outputs": [],
   "source": [
    "#now:\n",
    "df.dropna(axis=\"index\",how=\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhu8hxYVYVnJ"
   },
   "outputs": [],
   "source": [
    "df.isna()#Gives mask of which is NA or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TxUGaZQdYVnL"
   },
   "outputs": [],
   "source": [
    "#if Working with numerical data like stocks we would fill NA in with other data\n",
    "#ex:student didnt turn in assignment so is NA then we can give it 0 or some minimum marks using fillna method\n",
    "df.fillna(\"MISSING!!!\")#Fills in all NA values with things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqhqTtnyYVnM"
   },
   "source": [
    "### Casting Datatypes\n",
    "\n",
    "If we want to find avg age but the age columns is all strings so we need to cast it to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bNNb2oK7YVnM"
   },
   "outputs": [],
   "source": [
    "df.dtypes #age is object type as it has none also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GUtoqT44YVnN"
   },
   "outputs": [],
   "source": [
    "# df[\"Age\"].mean()\n",
    "#gives error as age is string type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2GyeJgwYVnO"
   },
   "source": [
    "**Note:We need to define age as float if we want to use NaN in it as np.nan is in fact a float data type.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YA8b3vwcYVnO"
   },
   "outputs": [],
   "source": [
    "type(np.nan)#nan has float type so we need to convert to float or else error will be there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vt3PSeEFYVnP"
   },
   "outputs": [],
   "source": [
    "# df[\"Age\"]=df[\"Age\"].astype(int)\n",
    "#Gives error as float not int\n",
    "df[\"Age\"]=df[\"Age\"].astype(float)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "faeNaXmmYVnQ"
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EU8634JyYVnR"
   },
   "outputs": [],
   "source": [
    "print(\"Mean Age is\",df[\"Age\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If working with pure numerical data we can also astype full dataframe also.\n",
    "\n",
    "Now let's load in the survey data. We can replace \"NA\" and \"Missing\" while loading in the data from file also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZBDK4BV-YVnT"
   },
   "outputs": [],
   "source": [
    "#Reloading Survey Data\n",
    "#Here in this survey there is no values that are wierd str\n",
    "na_vals=[\"NA\",\"Missing\"]\n",
    "df=pd.read_csv('survey_results_public.csv',index_col=\"Respondent\",na_values=na_vals)\n",
    "schema_df=pd.read_csv('survey_results_schema.csv',index_col=\"Column\",na_values=na_vals)\n",
    "pd.set_option(\"display.max_columns\",61)\n",
    "pd.set_option(\"display.max_rows\",61)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbNjvdH8YVnU"
   },
   "source": [
    "### Problems with Casting\n",
    "\n",
    "Let's try to calculate avg years of coding experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ei6IXNoKYVnV"
   },
   "outputs": [],
   "source": [
    "df[\"YearsCodePro\"].value_counts()#Total years of professional coding\n",
    "#Options are 1-50,less than 1 and more than 50\n",
    "#Hence, .mean() can't occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7y03O3JFYVnX"
   },
   "outputs": [],
   "source": [
    "#So some may try to cast it to float but less than 1 and more than cant be made to floats. Hence error\n",
    "# df[\"YearsCodePro\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EaVLwSrQYVnX"
   },
   "outputs": [],
   "source": [
    "#To just look at the unique valueswithout counting you can use the unique function\n",
    "df[\"YearsCodePro\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-RbMGuUYVnz"
   },
   "source": [
    "## Saving data in different file formats\n",
    "\n",
    "We have covered importing from different file formats. But writing to saif formats is also available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m6n_WjfBYVnz"
   },
   "outputs": [],
   "source": [
    "#df.to_csv(\"file_loc.csv\")           CSV FORMAT\n",
    "#df.to_csv(\"file_loc.tsv\",sep=\"\\t\")  TSV FORMAT\n",
    "#To save to excel you need ot install xlwt,openpyxl and xlrd with pip\n",
    "#df.to_excel(\"file_loc.xlsx\")        EXCEL FORMAT\n",
    "#df.to_json(\"file_loc.json\")         JSON FORMAT\n",
    "\n",
    "#There is also save to sql. but you need ot install SQLAlchemy and a communicator(psycopg for postgres) for storing\n",
    "#Then you need ot create an engine\n",
    "\n",
    "# from sqlalchemy import create_engine\n",
    "# import psycopg    (connector)\n",
    "# engine=create_engine(\"postgresql://user_name::pass@localhost:port/db_name\")\n",
    "\n",
    "#then you can save the data using the engine\n",
    "\n",
    "#df.to_sql(\"table_name\",engine,if_exists=\"replace\")\n",
    "\n",
    "#To read from SQL you ca use the same engine\n",
    "#df=pd.read_sql(\"df_name\",engine,index_col=\"Index_Col_Name\")\n",
    "\n",
    "#We can also make a query and take the output into dataframes\n",
    "#df=pd.read_sql_query(\"(query)SELECT .......\",engine,index_col=\"Index_Col_Name\")"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Varun Anand Patkar"
   }
  ],
  "colab": {
   "collapsed_sections": [],
   "name": "Pandas Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
